# Structure-Aware Backpropagation: Learning Neural Network Architectures Through Gradient-Based Structural Optimization

**Anonymous Authors**

## Abstract

Neural architecture search (NAS) has traditionally treated network topology as a discrete optimization problem, requiring separate search and training phases. We introduce Structure-Aware Backpropagation, a unified approach where network connectivity is parameterized by differentiable edge weights, enabling simultaneous optimization of architecture and parameters through standard gradient descent. Our method applies L1 regularization to edge weights, inducing automatic sparsification during training. We validate the approach on two tasks: XOR classification (74.5% accuracy, 8/12 active edges) and arithmetic addition regression (RÂ²=0.796, 12/16 active edges).
